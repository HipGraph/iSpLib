{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gBxcjRDEliK"
   },
   "source": [
    "# Graph Neural Networks\n",
    "\n",
    "In this tutorial, we will explore the implementation of graph neural networks and investigate what representations these networks learn. Along the way, we'll see how PyTorch Geometric and TensorBoardX can help us with constructing and training graph models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwdncyH6CEZ9"
   },
   "source": [
    "# Preliminaries: PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNtPXYKmCVow"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network\n",
    "# We first need to load the Cora dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "dataset = Planetoid(\"Planetoid\", name=\"Cora\", transform=T.ToSparseTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let’s implement a two-layer GCN\n",
    "The constructor defines two GCNConv layers which get called in the forward pass of our network. Note that the non-linearity is not integrated in the conv calls and hence needs to be applied afterwards (something which is consistent accross all operators in PyTorch Geometric). Here, we chose to use ReLU as our intermediate non-linearity between and finally output a softmax distribution over the number of classes. Let’s train this model on the train nodes for 200 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from the_sparse_package.tensor import SparseTensor\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16, cached=True)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes, cached=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, adj_t = data.x, data.adj_t\n",
    "        x = self.conv1(x, adj_t)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, adj_t)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let’s train this model on the train nodes for 200 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcn conv initialization\n",
      "gcn conv initialization\n",
      "gcn conv forward\n",
      "gcn conv message and aggregate\n",
      "aggr add\n",
      "inside the_sparse_package matmul function\n",
      "going into spspmm\n",
      "inside the sparse package spspmm function\n",
      "inside the spspmm_sum function\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Size mismatch: Expected size (2708, 1, ...) or (1, 16, ...), but got size torch.Size([16]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     out \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(out[data\u001b[39m.\u001b[39mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Github/SpGLL/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb Cell 10\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x, adj_t \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39madj_t\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, adj_t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gampa/Documents/Github/SpGLL/Graph_Neural_Networks.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/Documents/Github/SpGLL/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Github/SpGLL/pytorch_geometric/torch_geometric/nn/conv/gcn_conv.py:198\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    194\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mSparseTensor\u001b[39m.\u001b[39mfrom_dense(x), edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[1;32m    195\u001b[0m                      size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/Github/SpGLL/the_sparse_package/the_sparse_package/add.py:101\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39mset_value_(value, layout\u001b[39m=\u001b[39mlayout)\n\u001b[1;32m    100\u001b[0m SparseTensor\u001b[39m.\u001b[39madd \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, other: add(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m--> 101\u001b[0m SparseTensor\u001b[39m.\u001b[39madd_ \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, other: add_(\u001b[39mself\u001b[39;49m, other)\n\u001b[1;32m    102\u001b[0m SparseTensor\u001b[39m.\u001b[39madd_nnz \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, other, layout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m: add_nnz(\n\u001b[1;32m    103\u001b[0m     \u001b[39mself\u001b[39m, other, layout)\n\u001b[1;32m    104\u001b[0m SparseTensor\u001b[39m.\u001b[39madd_nnz_ \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39mself\u001b[39m, other, layout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m: add_nnz_(\n\u001b[1;32m    105\u001b[0m     \u001b[39mself\u001b[39m, other, layout)\n",
      "File \u001b[0;32m~/Documents/Github/SpGLL/the_sparse_package/the_sparse_package/add.py:69\u001b[0m, in \u001b[0;36madd_\u001b[0;34m(src, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m     other \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)[col]\n\u001b[1;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSize mismatch: Expected size (\u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, 1, ...) or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(1, \u001b[39m\u001b[39m{\u001b[39;00msrc\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, ...), but got size \u001b[39m\u001b[39m{\u001b[39;00mother\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39madd_(other\u001b[39m.\u001b[39mto(value\u001b[39m.\u001b[39mdtype))\n",
      "\u001b[0;31mValueError\u001b[0m: Size mismatch: Expected size (2708, 1, ...) or (1, 16, ...), but got size torch.Size([16])."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    _, pred = model(data).max(dim=1)\n",
    "    correct = float (pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "    acc = correct / data.train_mask.sum().item()\n",
    "    print('Epoch: %d, Accuracy: %.4f'%(epoch,acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we can evaluate our model on the test nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = model(data).max(dim=1)\n",
    "correct = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples: https://github.com/rusty1s/pytorch_geometric/tree/master/examples"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Graph Neural Networks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "caa1f377832983ad44480ba427bc9885089b5d20749e2ad716827f1f162f11e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
